{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JaNyjYRYbpSR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def landmarks_to_np(landmarks, dtype=\"int\"):\n",
    "    num = landmarks.num_parts\n",
    "    \n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((num, 2), dtype=dtype)\n",
    "    \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, num):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_centers(img, landmarks):\n",
    "    EYE_LEFT_OUTTER = landmarks[2]\n",
    "    EYE_LEFT_INNER = landmarks[3]\n",
    "    EYE_RIGHT_OUTTER = landmarks[0]\n",
    "    EYE_RIGHT_INNER = landmarks[1]\n",
    "\n",
    "    x = ((landmarks[0:4]).T)[0]\n",
    "    y = ((landmarks[0:4]).T)[1]\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    k, b = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    \n",
    "    x_left = (EYE_LEFT_OUTTER[0]+EYE_LEFT_INNER[0])/2\n",
    "    x_right = (EYE_RIGHT_OUTTER[0]+EYE_RIGHT_INNER[0])/2\n",
    "    LEFT_EYE_CENTER =  np.array([np.int32(x_left), np.int32(x_left*k+b)])\n",
    "    RIGHT_EYE_CENTER =  np.array([np.int32(x_right), np.int32(x_right*k+b)])\n",
    "    \n",
    "    pts = np.vstack((LEFT_EYE_CENTER,RIGHT_EYE_CENTER))\n",
    "    cv2.polylines(img, [pts], False, (255,0,0), 1)\n",
    "    cv2.circle(img, (LEFT_EYE_CENTER[0],LEFT_EYE_CENTER[1]), 3, (0, 0, 255), -1)\n",
    "    cv2.circle(img, (RIGHT_EYE_CENTER[0],RIGHT_EYE_CENTER[1]), 3, (0, 0, 255), -1)\n",
    "    \n",
    "    return LEFT_EYE_CENTER, RIGHT_EYE_CENTER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_aligned_face(img, left, right):\n",
    "    desired_w = 256\n",
    "    desired_h = 256\n",
    "    desired_dist = desired_w * 0.5\n",
    "\n",
    "    eyescenter = ((left[0]+right[0])*0.5 , (left[1]+right[1])*0.5)#between eyebrows\n",
    "    dx = right[0] - left[0]\n",
    "    dy = right[1] - left[1]\n",
    "    dist = np.sqrt(dx*dx + dy*dy)# interpupillary distance\n",
    "    scale = desired_dist / dist #scaling ratio\n",
    "    angle = np.degrees(np.arctan2(dy,dx)) # Rotation angle\n",
    "    M = cv2.getRotationMatrix2D(eyescenter,angle,scale)# Calculate the rotation matrix\n",
    "\n",
    "    # update the translation component of the matrix\n",
    "    tX = desired_w * 0.5\n",
    "    tY = desired_h * 0.5\n",
    "    M[0, 2] += (tX - eyescenter[0])\n",
    "    M[1, 2] += (tY - eyescenter[1])\n",
    "\n",
    "    aligned_face = cv2.warpAffine(img,M,(desired_w,desired_h))\n",
    "\n",
    "    return aligned_face"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def judge_eyeglass(img):\n",
    "    img = cv2.GaussianBlur(img, (11,11), 0) #Gaussian blur\n",
    "\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_64F, 0 ,1 , ksize=-1) #y-direction sobel edge detection\n",
    "    sobel_y = cv2.convertScaleAbs(sobel_y) #Convert back to uint8 type\n",
    "    cv2.imshow('sobel_y',sobel_y)\n",
    "\n",
    "    edgeness = sobel_y #edge strength matrix\n",
    "    \n",
    "    #Otsu binarization\n",
    "    retVal,thresh = cv2.threshold(edgeness,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    #Calculate feature length\n",
    "    d = len(thresh) * 0.5\n",
    "    x = np.int32(d * 6/7)\n",
    "    y = np.int32(d * 3/4)\n",
    "    w = np.int32(d * 2/7)\n",
    "    h = np.int32(d * 2/4)\n",
    "\n",
    "    x_2_1 = np.int32(d * 1/4)\n",
    "    x_2_2 = np.int32(d * 5/4)\n",
    "    w_2 = np.int32(d * 1/2)\n",
    "    y_2 = np.int32(d * 8/7)\n",
    "    h_2 = np.int32(d * 1/2)\n",
    "    \n",
    "    roi_1 = thresh[y:y+h, x:x+w] #Extract ROI\n",
    "    roi_2_1 = thresh[y_2:y_2+h_2, x_2_1:x_2_1+w_2]\n",
    "    roi_2_2 = thresh[y_2:y_2+h_2, x_2_2:x_2_2+w_2]\n",
    "    roi_2 = np.hstack([roi_2_1,roi_2_2])\n",
    "    \n",
    "    measure_1 = sum(sum(roi_1/255)) / (np.shape(roi_1)[0] * np.shape(roi_1)[1])#Calculate the evaluation value\n",
    "    measure_2 = sum(sum(roi_2/255)) / (np.shape(roi_2)[0] * np.shape(roi_2)[1])#Calculate the evaluation value\n",
    "    measure = measure_1*0.3 + measure_2*0.7\n",
    "    \n",
    "    # cv2.imshow('roi_1',roi_1)\n",
    "    # cv2.imshow('roi_2',roi_2)\n",
    "    print(measure)\n",
    "    \n",
    "    #Determine the discriminant value based on the relationship between the evaluation value and the threshold value\n",
    "    if measure > 0.15:#Threshold is adjustable, tested around 0.15\n",
    "        judge = True\n",
    "    else:\n",
    "        judge = False\n",
    "    print(judge)\n",
    "    return judge\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor_path = \"shape_predictor_5_face_landmarks.dat\"    #Face keypoint training data path\n",
    "detector = dlib.get_frontal_face_detector()#face detector\n",
    "predictor = dlib.shape_predictor(predictor_path)#Face Keypoint Detector\n",
    "\n",
    "cap = cv2.VideoCapture(0)#Turn on the camera\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    #read video frame\n",
    "    _, img = cap.read()\n",
    "    \n",
    "    #Convert to Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Face Detection\n",
    "    rects = detector(gray, 1)\n",
    "    \n",
    "    # operate on each detected face\n",
    "    for i, rect in enumerate(rects):\n",
    "        #get the coordinates\n",
    "        x_face = rect.left()\n",
    "        y_face = rect.top()\n",
    "        w_face = rect.right() - x_face\n",
    "        h_face = rect.bottom() - y_face\n",
    "        \n",
    "        #Draw borders, add text annotations\n",
    "        cv2.rectangle(img, (x_face,y_face), (x_face+w_face,y_face+h_face), (0,255,0), 2)\n",
    "        cv2.putText(img, \"Face #{}\".format(i + 1), (x_face - 10, y_face - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Detect and label landmarks     \n",
    "        landmarks = predictor(gray, rect)\n",
    "        landmarks = landmarks_to_np(landmarks)\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(img, (x, y), 2, (0, 0, 255), -1)\n",
    "\n",
    "        # Linear regression\n",
    "        LEFT_EYE_CENTER, RIGHT_EYE_CENTER = get_centers(img, landmarks)\n",
    "        \n",
    "        # face alignment\n",
    "        aligned_face = get_aligned_face(gray, LEFT_EYE_CENTER, RIGHT_EYE_CENTER)\n",
    "        cv2.imshow(\"aligned_face #{}\".format(i + 1), aligned_face)\n",
    "        \n",
    "        #Determine whether to wear glasses\n",
    "        judge = judge_eyeglass(aligned_face)\n",
    "        if judge == True:\n",
    "            cv2.putText(img, \"With Glasses\", (x_face + 100, y_face - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(img, \"No Glasses\", (x_face + 100, y_face - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # show result\n",
    "    cv2.imshow(\"Result\", img)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k==27:   #Press \"Esc\" to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a2d_iA8dagy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "71cf67cfa229d05aa3efe97f2e3a1e122474a824465fb6288c1e4a976dd48e65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}